# -*- coding: utf-8 -*-
"""4_Período.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1x9pnbz_A5Dv10oCjeFC1rhy7P5uc7qFv

# Pré - Processamento
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import matplotlib.dates as mdates
import math
import plotly.express as px
# %pylab inline

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv("/content/drive/MyDrive/IEEE/creditcard.csv")
df.head(3)

df.info()

nan = df.isna()
np.unique(nan.to_numpy(), return_counts=False) # obtendo os valores únicos de faltantes e não faltantes
                                              # convertendo da pandas para a numpy

"""# Normalização

"""

del df["Time"]

df.columns

df_1 = df.copy()

from sklearn import preprocessing

min_max_scaler = preprocessing.MinMaxScaler()

df = min_max_scaler.fit_transform(df)
print(df)

"""Dados normalizados."""

df = pd.DataFrame(df, columns=df_1.columns)
df.head(5)

df

df["Class"].value_counts()

"""#Perceptron from Scrath

Atividade Obrigatória

Classificação binária para prever fraudes nas transações com
cartões de crédito usando um Perceptron com uma camada
oculta feito somente com numpy:

1.   Separar a label das features e o dataset em subsets de
treinamento e teste;
2.   Inicialização randômica dos pesos;
3.   Definir a função de ativação e calcular sua derivada;
4.   Treinar o modelo testando diferentes valores de épocas
e
learning
rate,
identificando
quando
acontece
Overfitting e Underfitting.

5. Fazer as previsões nos dados de teste e avaliar o
modelo.
"""

from sklearn.model_selection import train_test_split

label = 'Class'
x = np.array(df.drop(label, axis=1))
y = np.array(df[label])

print(x.shape)
print(y.shape)

"""##Split Data"""

X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0 )

len(X_train[0])

"""1. Define a arquitetura da rede (quantas camadas, quantos neurônios em cada camada, função de ativação ...)
2. Os pesos são inicializados aleatoriamente
3. No processo de treinamento você vai ajustando os pesos com base nos dados com que você alimenta a rede (aqui entra backpropagation, gradient descent, loss)
4. Quando o treinamento finalizar, os pesos da sua rede ficam estáticos, ou seja, eles não mudam mais
5. Ao fazer uma previsão (alimentar a rede com dados que ela nunca viu e checar o resultado), o que é feito é apenas passar os dados pela rede (que já tem os pesos definidos) e verificar a saída
é por isso que em ML/DL o pré processamento dos dados é muito importante
Seja pra treinar ou prever
Uma vez que o treinamento acabou (que seria o .fit), os parâmetros do seu modelo são 'congelados'
então a rede sempre espera os dados em um determinado formato que foi determinado na definição da arquitetura/treinamento
"""

X_train.shape

"""Como temos 29 colunas, teremos 29 neurônios."""

from numpy import exp

weight = np.random.randn(29)

n=X_train.shape[1]
n

a =np.array([[1,2,3],[4,5,6]])
a.shape
print(a.reshape)

w1 = np.random.rand(29)  # Layer 1
b1 = np.random.rand(4)
w1

-w1

"""##Inicialização"""

class Inicialização():
  def __init__(self):
    pass
 
  def weight_bias(self, n_features, X):
    w1 = np.random.rand(n_features , X.shape[1])        # Layer 1
    b1 = np.random.rand(n_features)

    w2 = np.random.rand(n_features , X.shape[1])        # Layer 2
    b2 = np.random.rand(n_features)

    weight_and_bias = {"weight1": w1, "bias1": b1, "weight2": w2, "bias2": b2}
    return weight_and_bias

"""##Foward

"""

class Foward():

  def __init__(self):
    pass
        
  def propagation(self, x, weight_bias):
   

    z1 = np.dot(weight_bias["weight1"], x) + weight_bias["bias1"]        #Calculando os outputs da primeira camada
    z1_active = np.tanh(z1)  # O próximo passo é passar esses outputs ,da primeira camada, na função de ativação

    z2 = np.dot(weight_bias["weight2"], z1) + weight_bias["bias2"]       # Mesma descrição do comentário acima
    z2_active = self.sigmoid(z2)
    
    active = {"z1": z1 ,"z2": z2 , "z1_active": z1_active , "z2_active": z2_active}
    return z2_active, active


  def sigmoid(self,z):
      return 1/(1+np.exp(-z))

"""##Cost"""

# class Cost():
  
#   def __init__(self) -> None:
#       pass
def Entropy(active_layers, y):
  #Tem-se muitas funções para calcular o custo, e para cada modelo tem um tipo ideal de função. 
  #Por falta de espertise, escolhi a que seria mais fácil de ser implementada. Cross-Entropy-Loss-Function
  #Com uma rápida pesquisada pude perceber que essa função tem importância significativa na Teoria de Informação
  #Mas basicamente calcula-se a probabilidade da distribuição dos dados.  https://towardsdatascience.com/cross-entropy-loss-function-f38c4ec8643e
  # Compute the cross-entropy cost
  entropy_cost = np.multiply(np.log(active_layers), x) + np.multiply((1-x), np.log(1 - active_layers))

  cost = - np.sum(entropy_cost) / x.shape[1]
 
  cost1 = float(np.squeeze(cost))
                                  
  return cost1

"""##Backward"""

class Backward():
  def __init__(self):   #I belive that will not necessary index and xi
    pass
  
 
  def backpropagation(self, active_layers, weight_and_bias, x, y):
    
    n_features =29
#Para a última camada (layer) faremos a diferença entre o valor da camada na função de ativação
#(isso quer dizer ajustá-la num intervalo binário) menos o valor esperado, esses dados são a "diferença mínima" da layer
    dz2 = active_layers["z2_active"] - y
#Para os pesos multiplicamos esse "Diferença mínima" pela primeira camada transposta (aplicada na função de ativação) e dvidimos pela quantidade de neurônios
    dw2 = (dz2*active_layers["z1_active"])/n_features
#Para os bias tiramos a média dessa "Diferença mz2_activeínima"
    db2 = (np.sum(dz2, keepdims=True)) #Vale ressaltar que o "keepdims" mantém em formato de array

    dz1 = np.multiply(np.dot(weight_and_bias["weight2"].T, dz2), 1 - np.power(active_layers["z1_active"], 2))
    dw1 = (1/n_features)*np.dot(dz1, x.T)
    db1 = (1/n_features)*np.sum(dz1, keepdims=True)

    gradient = {"derivate_w1": dw1, "derivate_b1": db1, "derivate_w2": dw2, "derivate_b2": db2}

    return gradient

"""##Gradient"""

class Gradient():
  def __init__(self) -> None:
      pass

  def descent(self, gradient, weight_and_bias, learning_rate=0.01):
    w1 = weight_and_bias['weight1']
    b1 = weight_and_bias['bias1']
    w2 = weight_and_bias['weight2']
    b2 = weight_and_bias['bias2']
   
    dw1 = gradient['derivate_w1']
    db1 = gradient['derivate_b1']
    dw2 = gradient['derivate_w2']
    db2 = gradient['derivate_b2']

    w1 = w1 - learning_rate * dw1
    b1 = b1 - learning_rate * db1
    w2 = w2 - learning_rate * dw2
    b2 = b2 - learning_rate * db2
    
    weight_and_bias = {"weight1": w1, "bias1": b1, "weight2": w2, "bias2": b2}
    
    return weight_and_bias

"""##Perceptron

"""

class Perceptron():

  def __init__(self) -> None:

      self.weight_and_bias = None
      pass
        
  def fit(self, x, y, num_iterations = 1000):
    n_features = x.shape[1]
    init = Inicialização()
    self.weight_and_bias = init.weight_bias(n_features, x)
    
    w1 = self.weight_and_bias['weight1']
    b1 = self.weight_and_bias['bias1']
    w2 = self.weight_and_bias['weight2']
    b2 = self.weight_and_bias['bias2']
    
    forward = Foward()
    backward = Backward()
    gradient = Gradient()

    for i in range(0, num_iterations):

      z2_active, active = forward.propagation(x, self.weight_and_bias)

      cost = Entropy(z2_active, y)
      grads = backward.backpropagation(active, self.weight_and_bias, x, y)
      self.weight_and_bias = gradient.descent(grads, self.weight_and_bias)

    return self.weight_and_bias

  def prediction(self, x):
    forward = Foward()
    weight_bias, active = forward.propagation(x, self.weight_and_bias)
    predictions = np.round(active["z2_active"])
    
    return predictions

perceptron = Perceptron()

# perceptron.fit(X_train, y_test)

"""Nesse modelo, selecionarei somente uma parte do dataset para predição. Especificamente (29,29) do dataset.

##Selecionando aleatoriamente uma parte do data set
"""

# generate random integer values
from random import seed
from random import randint
# seed random number generator
random_value_test = randint(0,len(X_test))

# X_test = X_test[random_value_test:random_value_test+29]

# y_pred  =perceptron.prediction(X_test)

from sklearn.metrics import classification_report, r2_score, mean_squared_error

y_test = y_test[random_value_test:random_value_test+29]

# erro = np.power(abs(y_pred - y_test),2)

# mean_square = np.square(erro)
# mean_square = np.sum(mean_square)

"""Não consegui terminar o perceptron. Fui muito ousado ao tentar usar as classes, fico muito triste por não conseguir concluir. Mas colocarei aqui minhas análises a cerca do que eu esperava.

Ao analisar o dataset, pude perceber que a quantidade de valores 1 é maior que 0. 


1.   0.0  ---->    284315
2.   1.0  ---->     492

Para treinar esse dataset é necessário selecionar a mesma quantidade de zeros e uns. Porque daria overfitting.

#Perceptron by Sklearn
"""

from sklearn.linear_model import Perceptron
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

label = 'Class'
x = np.array(df.drop(label, axis=1))
y = np.array(df[label])

X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0 )

percep_x = Perceptron()

percep_x.fit(X_train, y_train)

y_pred = percep_x.predict(X_test)

def score_accuracy(array1, array2):
#Basicamente essa função accuracy ela soma a quantidade de elementos iguais e divide pelo tamanho , no nosso caso, y_test
  accuracy = np.sum(array1==array2) / len(array1)
  return accuracy

metrics_multiclass = classification_report(y_test, y_pred)

print(metrics_multiclass)

"""Podemos ver por esses dados, que o treino foi viciado, ou seja, o modelo está mais acostumado ao 0 do que o 1.

Undersampling
"""

df_new = df.copy()



aux = df_new.query('Class == 1')

X_1 = aux.drop('Class', axis = 1)

X_1 = np.array(X_1)



y_1 = aux[['Class', 'Amount']]

y_1 = y_1.drop('Amount', axis = 1)

y_1 = np.array(y_1)

value_zero = df.query("Class==0")
X_1 = value_zero.drop("Class", axis =1)
X_1 = np.array(X_1)

Y_1 = value_zero[['Class', 'Amount']]
Y_1 = Y_1.drop("Amount", axis =1)
Y_1 = np.array(Y_1)


value_one = df.query("Class==1")
X_2 = value_one.drop("Class", axis =1)
X_2 = np.array(X_2)

Y_2 = value_one[['Class', 'Amount']]
Y_2 = Y_2.drop("Amount", axis =1)
Y_2 = np.array(Y_2)

"""80% dos dados para treinar e os outros 20% para testar"""

X_train_2 = np.concatenate((X_1[:394], X_2[:394]))

X_test_2 = np.concatenate((X_1[:98], X_2[:98]))



Y_train_2 = np.concatenate((Y_1[:394], Y_2[:394]))

Y_test_2 = np.concatenate((Y_1[:98], Y_2[:98]))

percep_x = Perceptron()

percep_x.fit(X_train_2, Y_train_2)

y_pred = percep_x.predict(X_test_2)

metrics_multiclass = classification_report(Y_test_2, y_pred)

print(metrics_multiclass)

"""#Tensor Flow

Importando biblioteca
"""

import tensorflow as tf
import tensorflow.keras as keras

shape = X_train_2.shape

model = keras.models.Sequential()  # Cria uma rede neural sequencial - feed foward

model.add(keras.layers.Dense(2, activation='relu', input_shape=X_train_2[0].shape)) # primeira camada oculta com 2 neurônios

model.add(keras.layers.Dropout(0.2))

model.add(keras.layers.Dense(1, activation= 'relu')) # camada de saída com 1 neurônio com ativação sigmoid



model.summary()

"""Treino o a rede neural"""

model.compile(optimizer="sgd", loss="categorical_crossentropy", metrics=["binary_accuracy"])

X_train_2.shape

batch_size = 500  
epochs = 500 #Quantidade de iterações

history  = model.fit(X_train_2, Y_train_2, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(X_test_2, Y_test_2))

import matplotlib

import matplotlib.pyplot as plt



matplotlib.rcParams['figure.dpi'] = 150



# Plot acurácia de treino e teste

plt.plot(history.history['binary_accuracy'])

plt.plot(history.history['val_binary_accuracy'])

plt.title('Acurácia com Tensor FLow')

plt.ylabel('Acurácia')

plt.xlabel('Ciclos')

plt.legend(['Treino', 'Teste'], loc='upper left')

plt.show()

