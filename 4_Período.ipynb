{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4_Período.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "K6Sgb4R64yLF"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LucasLessa1/IEEE_CIS_Trainee/blob/main/4_Per%C3%ADodo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pré - Processamento\n"
      ],
      "metadata": {
        "id": "MWF8uY6QA1Up"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cSZTeK6d98yq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e958c25-df32-4f87-fe55-d8020bb5d627"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['datetime']\n",
            "`%matplotlib` prevents importing * from pylab and numpy\n",
            "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import matplotlib.dates as mdates\n",
        "import math\n",
        "import plotly.express as px\n",
        "%pylab inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dowE0ofk_zqY",
        "outputId": "ed1814e1-0954-4647-e270-e79c5e549fce"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/IEEE/creditcard.csv\")\n",
        "df.head(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "ONXtAZr_AI-6",
        "outputId": "8553c6de-b8e7-4919-9abd-5114276b8ed5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-445853a4-3368-4de2-b26b-e2be69218bea\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>V11</th>\n",
              "      <th>V12</th>\n",
              "      <th>V13</th>\n",
              "      <th>V14</th>\n",
              "      <th>V15</th>\n",
              "      <th>V16</th>\n",
              "      <th>V17</th>\n",
              "      <th>V18</th>\n",
              "      <th>V19</th>\n",
              "      <th>V20</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.359807</td>\n",
              "      <td>-0.072781</td>\n",
              "      <td>2.536347</td>\n",
              "      <td>1.378155</td>\n",
              "      <td>-0.338321</td>\n",
              "      <td>0.462388</td>\n",
              "      <td>0.239599</td>\n",
              "      <td>0.098698</td>\n",
              "      <td>0.363787</td>\n",
              "      <td>0.090794</td>\n",
              "      <td>-0.551600</td>\n",
              "      <td>-0.617801</td>\n",
              "      <td>-0.991390</td>\n",
              "      <td>-0.311169</td>\n",
              "      <td>1.468177</td>\n",
              "      <td>-0.470401</td>\n",
              "      <td>0.207971</td>\n",
              "      <td>0.025791</td>\n",
              "      <td>0.403993</td>\n",
              "      <td>0.251412</td>\n",
              "      <td>-0.018307</td>\n",
              "      <td>0.277838</td>\n",
              "      <td>-0.110474</td>\n",
              "      <td>0.066928</td>\n",
              "      <td>0.128539</td>\n",
              "      <td>-0.189115</td>\n",
              "      <td>0.133558</td>\n",
              "      <td>-0.021053</td>\n",
              "      <td>149.62</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.191857</td>\n",
              "      <td>0.266151</td>\n",
              "      <td>0.166480</td>\n",
              "      <td>0.448154</td>\n",
              "      <td>0.060018</td>\n",
              "      <td>-0.082361</td>\n",
              "      <td>-0.078803</td>\n",
              "      <td>0.085102</td>\n",
              "      <td>-0.255425</td>\n",
              "      <td>-0.166974</td>\n",
              "      <td>1.612727</td>\n",
              "      <td>1.065235</td>\n",
              "      <td>0.489095</td>\n",
              "      <td>-0.143772</td>\n",
              "      <td>0.635558</td>\n",
              "      <td>0.463917</td>\n",
              "      <td>-0.114805</td>\n",
              "      <td>-0.183361</td>\n",
              "      <td>-0.145783</td>\n",
              "      <td>-0.069083</td>\n",
              "      <td>-0.225775</td>\n",
              "      <td>-0.638672</td>\n",
              "      <td>0.101288</td>\n",
              "      <td>-0.339846</td>\n",
              "      <td>0.167170</td>\n",
              "      <td>0.125895</td>\n",
              "      <td>-0.008983</td>\n",
              "      <td>0.014724</td>\n",
              "      <td>2.69</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.358354</td>\n",
              "      <td>-1.340163</td>\n",
              "      <td>1.773209</td>\n",
              "      <td>0.379780</td>\n",
              "      <td>-0.503198</td>\n",
              "      <td>1.800499</td>\n",
              "      <td>0.791461</td>\n",
              "      <td>0.247676</td>\n",
              "      <td>-1.514654</td>\n",
              "      <td>0.207643</td>\n",
              "      <td>0.624501</td>\n",
              "      <td>0.066084</td>\n",
              "      <td>0.717293</td>\n",
              "      <td>-0.165946</td>\n",
              "      <td>2.345865</td>\n",
              "      <td>-2.890083</td>\n",
              "      <td>1.109969</td>\n",
              "      <td>-0.121359</td>\n",
              "      <td>-2.261857</td>\n",
              "      <td>0.524980</td>\n",
              "      <td>0.247998</td>\n",
              "      <td>0.771679</td>\n",
              "      <td>0.909412</td>\n",
              "      <td>-0.689281</td>\n",
              "      <td>-0.327642</td>\n",
              "      <td>-0.139097</td>\n",
              "      <td>-0.055353</td>\n",
              "      <td>-0.059752</td>\n",
              "      <td>378.66</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-445853a4-3368-4de2-b26b-e2be69218bea')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-445853a4-3368-4de2-b26b-e2be69218bea button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-445853a4-3368-4de2-b26b-e2be69218bea');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   Time        V1        V2        V3  ...       V27       V28  Amount  Class\n",
              "0   0.0 -1.359807 -0.072781  2.536347  ...  0.133558 -0.021053  149.62      0\n",
              "1   0.0  1.191857  0.266151  0.166480  ... -0.008983  0.014724    2.69      0\n",
              "2   1.0 -1.358354 -1.340163  1.773209  ... -0.055353 -0.059752  378.66      0\n",
              "\n",
              "[3 rows x 31 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "-GdigYBCAzYD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10750461-6886-40bd-9722-6dbe775bec66"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 284807 entries, 0 to 284806\n",
            "Data columns (total 31 columns):\n",
            " #   Column  Non-Null Count   Dtype  \n",
            "---  ------  --------------   -----  \n",
            " 0   Time    284807 non-null  float64\n",
            " 1   V1      284807 non-null  float64\n",
            " 2   V2      284807 non-null  float64\n",
            " 3   V3      284807 non-null  float64\n",
            " 4   V4      284807 non-null  float64\n",
            " 5   V5      284807 non-null  float64\n",
            " 6   V6      284807 non-null  float64\n",
            " 7   V7      284807 non-null  float64\n",
            " 8   V8      284807 non-null  float64\n",
            " 9   V9      284807 non-null  float64\n",
            " 10  V10     284807 non-null  float64\n",
            " 11  V11     284807 non-null  float64\n",
            " 12  V12     284807 non-null  float64\n",
            " 13  V13     284807 non-null  float64\n",
            " 14  V14     284807 non-null  float64\n",
            " 15  V15     284807 non-null  float64\n",
            " 16  V16     284807 non-null  float64\n",
            " 17  V17     284807 non-null  float64\n",
            " 18  V18     284807 non-null  float64\n",
            " 19  V19     284807 non-null  float64\n",
            " 20  V20     284807 non-null  float64\n",
            " 21  V21     284807 non-null  float64\n",
            " 22  V22     284807 non-null  float64\n",
            " 23  V23     284807 non-null  float64\n",
            " 24  V24     284807 non-null  float64\n",
            " 25  V25     284807 non-null  float64\n",
            " 26  V26     284807 non-null  float64\n",
            " 27  V27     284807 non-null  float64\n",
            " 28  V28     284807 non-null  float64\n",
            " 29  Amount  284807 non-null  float64\n",
            " 30  Class   284807 non-null  int64  \n",
            "dtypes: float64(30), int64(1)\n",
            "memory usage: 67.4 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nan = df.isna()\n",
        "np.unique(nan.to_numpy(), return_counts=False) # obtendo os valores únicos de faltantes e não faltantes\n",
        "                                              # convertendo da pandas para a numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTNvB-IszvcE",
        "outputId": "0308def5-7d4b-4386-d771-604550670589"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([False])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Normalização\n"
      ],
      "metadata": {
        "id": "DR4NDYVC0iRr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "del df[\"Time\"]"
      ],
      "metadata": {
        "id": "JgK8rK5kfmgQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipcRdDcJfrwq",
        "outputId": "c4a4b6b5-a199-428b-c459-31e5599278a7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11',\n",
              "       'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21',\n",
              "       'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount', 'Class'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_1 = df.copy()"
      ],
      "metadata": {
        "id": "zzWBLbuU0Ih3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "min_max_scaler = preprocessing.MinMaxScaler()"
      ],
      "metadata": {
        "id": "dfUYJOuE1J1Y"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = min_max_scaler.fit_transform(df)\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwAlY9Z71MQC",
        "outputId": "6ea8f270-e2c4-4add-a75d-e37b3ef62781"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[9.35192337e-01 7.66490419e-01 8.81364903e-01 ... 3.12696634e-01\n",
            "  5.82379309e-03 0.00000000e+00]\n",
            " [9.78541955e-01 7.70066651e-01 8.40298490e-01 ... 3.13422663e-01\n",
            "  1.04705276e-04 0.00000000e+00]\n",
            " [9.35217023e-01 7.53117667e-01 8.68140819e-01 ... 3.11911316e-01\n",
            "  1.47389219e-02 0.00000000e+00]\n",
            " ...\n",
            " [9.90904812e-01 7.64079694e-01 7.81101998e-01 ... 3.12584864e-01\n",
            "  2.64215395e-03 0.00000000e+00]\n",
            " [9.54208999e-01 7.72855742e-01 8.49587129e-01 ... 3.15245157e-01\n",
            "  3.89238944e-04 0.00000000e+00]\n",
            " [9.49231759e-01 7.65256401e-01 8.49601462e-01 ... 3.13400843e-01\n",
            "  8.44648509e-03 0.00000000e+00]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dados normalizados."
      ],
      "metadata": {
        "id": "kAl777ep151U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(df, columns=df_1.columns)\n",
        "df.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "_8jn1b-c1V-E",
        "outputId": "4b350654-6f59-45b2-c747-e97bb11fad27"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-deff7e93-4890-417e-be4d-dfe559a039fe\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>V11</th>\n",
              "      <th>V12</th>\n",
              "      <th>V13</th>\n",
              "      <th>V14</th>\n",
              "      <th>V15</th>\n",
              "      <th>V16</th>\n",
              "      <th>V17</th>\n",
              "      <th>V18</th>\n",
              "      <th>V19</th>\n",
              "      <th>V20</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.935192</td>\n",
              "      <td>0.766490</td>\n",
              "      <td>0.881365</td>\n",
              "      <td>0.313023</td>\n",
              "      <td>0.763439</td>\n",
              "      <td>0.267669</td>\n",
              "      <td>0.266815</td>\n",
              "      <td>0.786444</td>\n",
              "      <td>0.475312</td>\n",
              "      <td>0.510600</td>\n",
              "      <td>0.252484</td>\n",
              "      <td>0.680908</td>\n",
              "      <td>0.371591</td>\n",
              "      <td>0.635591</td>\n",
              "      <td>0.446084</td>\n",
              "      <td>0.434392</td>\n",
              "      <td>0.737173</td>\n",
              "      <td>0.655066</td>\n",
              "      <td>0.594863</td>\n",
              "      <td>0.582942</td>\n",
              "      <td>0.561184</td>\n",
              "      <td>0.522992</td>\n",
              "      <td>0.663793</td>\n",
              "      <td>0.391253</td>\n",
              "      <td>0.585122</td>\n",
              "      <td>0.394557</td>\n",
              "      <td>0.418976</td>\n",
              "      <td>0.312697</td>\n",
              "      <td>0.005824</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.978542</td>\n",
              "      <td>0.770067</td>\n",
              "      <td>0.840298</td>\n",
              "      <td>0.271796</td>\n",
              "      <td>0.766120</td>\n",
              "      <td>0.262192</td>\n",
              "      <td>0.264875</td>\n",
              "      <td>0.786298</td>\n",
              "      <td>0.453981</td>\n",
              "      <td>0.505267</td>\n",
              "      <td>0.381188</td>\n",
              "      <td>0.744342</td>\n",
              "      <td>0.486190</td>\n",
              "      <td>0.641219</td>\n",
              "      <td>0.383840</td>\n",
              "      <td>0.464105</td>\n",
              "      <td>0.727794</td>\n",
              "      <td>0.640681</td>\n",
              "      <td>0.551930</td>\n",
              "      <td>0.579530</td>\n",
              "      <td>0.557840</td>\n",
              "      <td>0.480237</td>\n",
              "      <td>0.666938</td>\n",
              "      <td>0.336440</td>\n",
              "      <td>0.587290</td>\n",
              "      <td>0.446013</td>\n",
              "      <td>0.416345</td>\n",
              "      <td>0.313423</td>\n",
              "      <td>0.000105</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.935217</td>\n",
              "      <td>0.753118</td>\n",
              "      <td>0.868141</td>\n",
              "      <td>0.268766</td>\n",
              "      <td>0.762329</td>\n",
              "      <td>0.281122</td>\n",
              "      <td>0.270177</td>\n",
              "      <td>0.788042</td>\n",
              "      <td>0.410603</td>\n",
              "      <td>0.513018</td>\n",
              "      <td>0.322422</td>\n",
              "      <td>0.706683</td>\n",
              "      <td>0.503854</td>\n",
              "      <td>0.640473</td>\n",
              "      <td>0.511697</td>\n",
              "      <td>0.357443</td>\n",
              "      <td>0.763381</td>\n",
              "      <td>0.644945</td>\n",
              "      <td>0.386683</td>\n",
              "      <td>0.585855</td>\n",
              "      <td>0.565477</td>\n",
              "      <td>0.546030</td>\n",
              "      <td>0.678939</td>\n",
              "      <td>0.289354</td>\n",
              "      <td>0.559515</td>\n",
              "      <td>0.402727</td>\n",
              "      <td>0.415489</td>\n",
              "      <td>0.311911</td>\n",
              "      <td>0.014739</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.941878</td>\n",
              "      <td>0.765304</td>\n",
              "      <td>0.868484</td>\n",
              "      <td>0.213661</td>\n",
              "      <td>0.765647</td>\n",
              "      <td>0.275559</td>\n",
              "      <td>0.266803</td>\n",
              "      <td>0.789434</td>\n",
              "      <td>0.414999</td>\n",
              "      <td>0.507585</td>\n",
              "      <td>0.271817</td>\n",
              "      <td>0.710910</td>\n",
              "      <td>0.487635</td>\n",
              "      <td>0.636372</td>\n",
              "      <td>0.289124</td>\n",
              "      <td>0.415653</td>\n",
              "      <td>0.711253</td>\n",
              "      <td>0.788492</td>\n",
              "      <td>0.467058</td>\n",
              "      <td>0.578050</td>\n",
              "      <td>0.559734</td>\n",
              "      <td>0.510277</td>\n",
              "      <td>0.662607</td>\n",
              "      <td>0.223826</td>\n",
              "      <td>0.614245</td>\n",
              "      <td>0.389197</td>\n",
              "      <td>0.417669</td>\n",
              "      <td>0.314371</td>\n",
              "      <td>0.004807</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.938617</td>\n",
              "      <td>0.776520</td>\n",
              "      <td>0.864251</td>\n",
              "      <td>0.269796</td>\n",
              "      <td>0.762975</td>\n",
              "      <td>0.263984</td>\n",
              "      <td>0.268968</td>\n",
              "      <td>0.782484</td>\n",
              "      <td>0.490950</td>\n",
              "      <td>0.524303</td>\n",
              "      <td>0.236355</td>\n",
              "      <td>0.724477</td>\n",
              "      <td>0.552509</td>\n",
              "      <td>0.608406</td>\n",
              "      <td>0.349419</td>\n",
              "      <td>0.434995</td>\n",
              "      <td>0.724243</td>\n",
              "      <td>0.650665</td>\n",
              "      <td>0.626060</td>\n",
              "      <td>0.584615</td>\n",
              "      <td>0.561327</td>\n",
              "      <td>0.547271</td>\n",
              "      <td>0.663392</td>\n",
              "      <td>0.401270</td>\n",
              "      <td>0.566343</td>\n",
              "      <td>0.507497</td>\n",
              "      <td>0.420561</td>\n",
              "      <td>0.317490</td>\n",
              "      <td>0.002724</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-deff7e93-4890-417e-be4d-dfe559a039fe')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-deff7e93-4890-417e-be4d-dfe559a039fe button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-deff7e93-4890-417e-be4d-dfe559a039fe');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "         V1        V2        V3        V4  ...       V27       V28    Amount  Class\n",
              "0  0.935192  0.766490  0.881365  0.313023  ...  0.418976  0.312697  0.005824    0.0\n",
              "1  0.978542  0.770067  0.840298  0.271796  ...  0.416345  0.313423  0.000105    0.0\n",
              "2  0.935217  0.753118  0.868141  0.268766  ...  0.415489  0.311911  0.014739    0.0\n",
              "3  0.941878  0.765304  0.868484  0.213661  ...  0.417669  0.314371  0.004807    0.0\n",
              "4  0.938617  0.776520  0.864251  0.269796  ...  0.420561  0.317490  0.002724    0.0\n",
              "\n",
              "[5 rows x 30 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "dkMbmRxybdTO",
        "outputId": "053add55-ce31-4c77-d5ea-4eb231085adb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-f682815a-354b-4be9-9325-6ec7dd8ee6ec\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>V11</th>\n",
              "      <th>V12</th>\n",
              "      <th>V13</th>\n",
              "      <th>V14</th>\n",
              "      <th>V15</th>\n",
              "      <th>V16</th>\n",
              "      <th>V17</th>\n",
              "      <th>V18</th>\n",
              "      <th>V19</th>\n",
              "      <th>V20</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.935192</td>\n",
              "      <td>0.766490</td>\n",
              "      <td>0.881365</td>\n",
              "      <td>0.313023</td>\n",
              "      <td>0.763439</td>\n",
              "      <td>0.267669</td>\n",
              "      <td>0.266815</td>\n",
              "      <td>0.786444</td>\n",
              "      <td>0.475312</td>\n",
              "      <td>0.510600</td>\n",
              "      <td>0.252484</td>\n",
              "      <td>0.680908</td>\n",
              "      <td>0.371591</td>\n",
              "      <td>0.635591</td>\n",
              "      <td>0.446084</td>\n",
              "      <td>0.434392</td>\n",
              "      <td>0.737173</td>\n",
              "      <td>0.655066</td>\n",
              "      <td>0.594863</td>\n",
              "      <td>0.582942</td>\n",
              "      <td>0.561184</td>\n",
              "      <td>0.522992</td>\n",
              "      <td>0.663793</td>\n",
              "      <td>0.391253</td>\n",
              "      <td>0.585122</td>\n",
              "      <td>0.394557</td>\n",
              "      <td>0.418976</td>\n",
              "      <td>0.312697</td>\n",
              "      <td>0.005824</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.978542</td>\n",
              "      <td>0.770067</td>\n",
              "      <td>0.840298</td>\n",
              "      <td>0.271796</td>\n",
              "      <td>0.766120</td>\n",
              "      <td>0.262192</td>\n",
              "      <td>0.264875</td>\n",
              "      <td>0.786298</td>\n",
              "      <td>0.453981</td>\n",
              "      <td>0.505267</td>\n",
              "      <td>0.381188</td>\n",
              "      <td>0.744342</td>\n",
              "      <td>0.486190</td>\n",
              "      <td>0.641219</td>\n",
              "      <td>0.383840</td>\n",
              "      <td>0.464105</td>\n",
              "      <td>0.727794</td>\n",
              "      <td>0.640681</td>\n",
              "      <td>0.551930</td>\n",
              "      <td>0.579530</td>\n",
              "      <td>0.557840</td>\n",
              "      <td>0.480237</td>\n",
              "      <td>0.666938</td>\n",
              "      <td>0.336440</td>\n",
              "      <td>0.587290</td>\n",
              "      <td>0.446013</td>\n",
              "      <td>0.416345</td>\n",
              "      <td>0.313423</td>\n",
              "      <td>0.000105</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.935217</td>\n",
              "      <td>0.753118</td>\n",
              "      <td>0.868141</td>\n",
              "      <td>0.268766</td>\n",
              "      <td>0.762329</td>\n",
              "      <td>0.281122</td>\n",
              "      <td>0.270177</td>\n",
              "      <td>0.788042</td>\n",
              "      <td>0.410603</td>\n",
              "      <td>0.513018</td>\n",
              "      <td>0.322422</td>\n",
              "      <td>0.706683</td>\n",
              "      <td>0.503854</td>\n",
              "      <td>0.640473</td>\n",
              "      <td>0.511697</td>\n",
              "      <td>0.357443</td>\n",
              "      <td>0.763381</td>\n",
              "      <td>0.644945</td>\n",
              "      <td>0.386683</td>\n",
              "      <td>0.585855</td>\n",
              "      <td>0.565477</td>\n",
              "      <td>0.546030</td>\n",
              "      <td>0.678939</td>\n",
              "      <td>0.289354</td>\n",
              "      <td>0.559515</td>\n",
              "      <td>0.402727</td>\n",
              "      <td>0.415489</td>\n",
              "      <td>0.311911</td>\n",
              "      <td>0.014739</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.941878</td>\n",
              "      <td>0.765304</td>\n",
              "      <td>0.868484</td>\n",
              "      <td>0.213661</td>\n",
              "      <td>0.765647</td>\n",
              "      <td>0.275559</td>\n",
              "      <td>0.266803</td>\n",
              "      <td>0.789434</td>\n",
              "      <td>0.414999</td>\n",
              "      <td>0.507585</td>\n",
              "      <td>0.271817</td>\n",
              "      <td>0.710910</td>\n",
              "      <td>0.487635</td>\n",
              "      <td>0.636372</td>\n",
              "      <td>0.289124</td>\n",
              "      <td>0.415653</td>\n",
              "      <td>0.711253</td>\n",
              "      <td>0.788492</td>\n",
              "      <td>0.467058</td>\n",
              "      <td>0.578050</td>\n",
              "      <td>0.559734</td>\n",
              "      <td>0.510277</td>\n",
              "      <td>0.662607</td>\n",
              "      <td>0.223826</td>\n",
              "      <td>0.614245</td>\n",
              "      <td>0.389197</td>\n",
              "      <td>0.417669</td>\n",
              "      <td>0.314371</td>\n",
              "      <td>0.004807</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.938617</td>\n",
              "      <td>0.776520</td>\n",
              "      <td>0.864251</td>\n",
              "      <td>0.269796</td>\n",
              "      <td>0.762975</td>\n",
              "      <td>0.263984</td>\n",
              "      <td>0.268968</td>\n",
              "      <td>0.782484</td>\n",
              "      <td>0.490950</td>\n",
              "      <td>0.524303</td>\n",
              "      <td>0.236355</td>\n",
              "      <td>0.724477</td>\n",
              "      <td>0.552509</td>\n",
              "      <td>0.608406</td>\n",
              "      <td>0.349419</td>\n",
              "      <td>0.434995</td>\n",
              "      <td>0.724243</td>\n",
              "      <td>0.650665</td>\n",
              "      <td>0.626060</td>\n",
              "      <td>0.584615</td>\n",
              "      <td>0.561327</td>\n",
              "      <td>0.547271</td>\n",
              "      <td>0.663392</td>\n",
              "      <td>0.401270</td>\n",
              "      <td>0.566343</td>\n",
              "      <td>0.507497</td>\n",
              "      <td>0.420561</td>\n",
              "      <td>0.317490</td>\n",
              "      <td>0.002724</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284802</th>\n",
              "      <td>0.756448</td>\n",
              "      <td>0.873531</td>\n",
              "      <td>0.666991</td>\n",
              "      <td>0.160317</td>\n",
              "      <td>0.729603</td>\n",
              "      <td>0.236810</td>\n",
              "      <td>0.235393</td>\n",
              "      <td>0.863749</td>\n",
              "      <td>0.528729</td>\n",
              "      <td>0.598850</td>\n",
              "      <td>0.190550</td>\n",
              "      <td>0.806406</td>\n",
              "      <td>0.394978</td>\n",
              "      <td>0.801627</td>\n",
              "      <td>0.267218</td>\n",
              "      <td>0.484577</td>\n",
              "      <td>0.789000</td>\n",
              "      <td>0.688412</td>\n",
              "      <td>0.509985</td>\n",
              "      <td>0.595979</td>\n",
              "      <td>0.564920</td>\n",
              "      <td>0.515249</td>\n",
              "      <td>0.680500</td>\n",
              "      <td>0.313600</td>\n",
              "      <td>0.658558</td>\n",
              "      <td>0.466291</td>\n",
              "      <td>0.433929</td>\n",
              "      <td>0.329840</td>\n",
              "      <td>0.000030</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284803</th>\n",
              "      <td>0.945845</td>\n",
              "      <td>0.766677</td>\n",
              "      <td>0.872678</td>\n",
              "      <td>0.219189</td>\n",
              "      <td>0.771561</td>\n",
              "      <td>0.273661</td>\n",
              "      <td>0.265504</td>\n",
              "      <td>0.788548</td>\n",
              "      <td>0.482925</td>\n",
              "      <td>0.488530</td>\n",
              "      <td>0.276355</td>\n",
              "      <td>0.738709</td>\n",
              "      <td>0.542361</td>\n",
              "      <td>0.623352</td>\n",
              "      <td>0.423414</td>\n",
              "      <td>0.426717</td>\n",
              "      <td>0.730383</td>\n",
              "      <td>0.569303</td>\n",
              "      <td>0.442620</td>\n",
              "      <td>0.580900</td>\n",
              "      <td>0.564933</td>\n",
              "      <td>0.553153</td>\n",
              "      <td>0.665619</td>\n",
              "      <td>0.245298</td>\n",
              "      <td>0.543855</td>\n",
              "      <td>0.360884</td>\n",
              "      <td>0.417775</td>\n",
              "      <td>0.312038</td>\n",
              "      <td>0.000965</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284804</th>\n",
              "      <td>0.990905</td>\n",
              "      <td>0.764080</td>\n",
              "      <td>0.781102</td>\n",
              "      <td>0.227202</td>\n",
              "      <td>0.783425</td>\n",
              "      <td>0.293496</td>\n",
              "      <td>0.263547</td>\n",
              "      <td>0.792985</td>\n",
              "      <td>0.477677</td>\n",
              "      <td>0.498692</td>\n",
              "      <td>0.309763</td>\n",
              "      <td>0.706572</td>\n",
              "      <td>0.434111</td>\n",
              "      <td>0.628885</td>\n",
              "      <td>0.435700</td>\n",
              "      <td>0.453827</td>\n",
              "      <td>0.740239</td>\n",
              "      <td>0.680504</td>\n",
              "      <td>0.518236</td>\n",
              "      <td>0.580280</td>\n",
              "      <td>0.565220</td>\n",
              "      <td>0.537005</td>\n",
              "      <td>0.664877</td>\n",
              "      <td>0.468492</td>\n",
              "      <td>0.592824</td>\n",
              "      <td>0.411177</td>\n",
              "      <td>0.416593</td>\n",
              "      <td>0.312585</td>\n",
              "      <td>0.002642</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284805</th>\n",
              "      <td>0.954209</td>\n",
              "      <td>0.772856</td>\n",
              "      <td>0.849587</td>\n",
              "      <td>0.282508</td>\n",
              "      <td>0.763172</td>\n",
              "      <td>0.269291</td>\n",
              "      <td>0.261175</td>\n",
              "      <td>0.792671</td>\n",
              "      <td>0.476287</td>\n",
              "      <td>0.500464</td>\n",
              "      <td>0.170288</td>\n",
              "      <td>0.667901</td>\n",
              "      <td>0.367667</td>\n",
              "      <td>0.661171</td>\n",
              "      <td>0.483042</td>\n",
              "      <td>0.429998</td>\n",
              "      <td>0.745946</td>\n",
              "      <td>0.729908</td>\n",
              "      <td>0.789612</td>\n",
              "      <td>0.581622</td>\n",
              "      <td>0.565755</td>\n",
              "      <td>0.547353</td>\n",
              "      <td>0.663008</td>\n",
              "      <td>0.398836</td>\n",
              "      <td>0.545958</td>\n",
              "      <td>0.514746</td>\n",
              "      <td>0.418520</td>\n",
              "      <td>0.315245</td>\n",
              "      <td>0.000389</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284806</th>\n",
              "      <td>0.949232</td>\n",
              "      <td>0.765256</td>\n",
              "      <td>0.849601</td>\n",
              "      <td>0.229488</td>\n",
              "      <td>0.765632</td>\n",
              "      <td>0.256488</td>\n",
              "      <td>0.274963</td>\n",
              "      <td>0.780938</td>\n",
              "      <td>0.479528</td>\n",
              "      <td>0.489782</td>\n",
              "      <td>0.223414</td>\n",
              "      <td>0.703005</td>\n",
              "      <td>0.433771</td>\n",
              "      <td>0.643218</td>\n",
              "      <td>0.339417</td>\n",
              "      <td>0.439728</td>\n",
              "      <td>0.711942</td>\n",
              "      <td>0.664807</td>\n",
              "      <td>0.543314</td>\n",
              "      <td>0.584343</td>\n",
              "      <td>0.565688</td>\n",
              "      <td>0.540031</td>\n",
              "      <td>0.671029</td>\n",
              "      <td>0.383420</td>\n",
              "      <td>0.551319</td>\n",
              "      <td>0.291786</td>\n",
              "      <td>0.416466</td>\n",
              "      <td>0.313401</td>\n",
              "      <td>0.008446</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>284807 rows × 30 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f682815a-354b-4be9-9325-6ec7dd8ee6ec')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f682815a-354b-4be9-9325-6ec7dd8ee6ec button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f682815a-354b-4be9-9325-6ec7dd8ee6ec');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "              V1        V2        V3  ...       V28    Amount  Class\n",
              "0       0.935192  0.766490  0.881365  ...  0.312697  0.005824    0.0\n",
              "1       0.978542  0.770067  0.840298  ...  0.313423  0.000105    0.0\n",
              "2       0.935217  0.753118  0.868141  ...  0.311911  0.014739    0.0\n",
              "3       0.941878  0.765304  0.868484  ...  0.314371  0.004807    0.0\n",
              "4       0.938617  0.776520  0.864251  ...  0.317490  0.002724    0.0\n",
              "...          ...       ...       ...  ...       ...       ...    ...\n",
              "284802  0.756448  0.873531  0.666991  ...  0.329840  0.000030    0.0\n",
              "284803  0.945845  0.766677  0.872678  ...  0.312038  0.000965    0.0\n",
              "284804  0.990905  0.764080  0.781102  ...  0.312585  0.002642    0.0\n",
              "284805  0.954209  0.772856  0.849587  ...  0.315245  0.000389    0.0\n",
              "284806  0.949232  0.765256  0.849601  ...  0.313401  0.008446    0.0\n",
              "\n",
              "[284807 rows x 30 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Class\"].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPpyTnHwF4ou",
        "outputId": "dcb66432-eee9-4168-cb23-d96093312119"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0    284315\n",
              "1.0       492\n",
              "Name: Class, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Perceptron from Scrath"
      ],
      "metadata": {
        "id": "d9XyR40-2FoL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Atividade Obrigatória"
      ],
      "metadata": {
        "id": "KdC1q41h2fXJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classificação binária para prever fraudes nas transações com\n",
        "cartões de crédito usando um Perceptron com uma camada\n",
        "oculta feito somente com numpy:\n",
        "\n",
        "1.   Separar a label das features e o dataset em subsets de\n",
        "treinamento e teste;\n",
        "2.   Inicialização randômica dos pesos;\n",
        "3.   Definir a função de ativação e calcular sua derivada;\n",
        "4.   Treinar o modelo testando diferentes valores de épocas\n",
        "e\n",
        "learning\n",
        "rate,\n",
        "identificando\n",
        "quando\n",
        "acontece\n",
        "Overfitting e Underfitting.\n",
        "\n",
        "5. Fazer as previsões nos dados de teste e avaliar o\n",
        "modelo."
      ],
      "metadata": {
        "id": "wFWGMFAu2JHS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "uThwulpu83Ma"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label = 'Class'\n",
        "x = np.array(df.drop(label, axis=1))\n",
        "y = np.array(df[label])"
      ],
      "metadata": {
        "id": "wsT_1Wvl9BcI"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x.shape)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLP_b8Ml8jQu",
        "outputId": "b5c936ef-ecc1-43b9-a2b7-a2ffbdd13aa6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(284807, 29)\n",
            "(284807,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Split Data"
      ],
      "metadata": {
        "id": "PKV3OpFb0MJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0 )"
      ],
      "metadata": {
        "id": "8HM-D8Pp-ZT_"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbQZknT8Utmn",
        "outputId": "368eaf91-2ae1-4d37-92ef-5b114df621d3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Define a arquitetura da rede (quantas camadas, quantos neurônios em cada camada, função de ativação ...)\n",
        "2. Os pesos são inicializados aleatoriamente\n",
        "3. No processo de treinamento você vai ajustando os pesos com base nos dados com que você alimenta a rede (aqui entra backpropagation, gradient descent, loss)\n",
        "4. Quando o treinamento finalizar, os pesos da sua rede ficam estáticos, ou seja, eles não mudam mais\n",
        "5. Ao fazer uma previsão (alimentar a rede com dados que ela nunca viu e checar o resultado), o que é feito é apenas passar os dados pela rede (que já tem os pesos definidos) e verificar a saída\n",
        "é por isso que em ML/DL o pré processamento dos dados é muito importante\n",
        "Seja pra treinar ou prever\n",
        "Uma vez que o treinamento acabou (que seria o .fit), os parâmetros do seu modelo são 'congelados'\n",
        "então a rede sempre espera os dados em um determinado formato que foi determinado na definição da arquitetura/treinamento"
      ],
      "metadata": {
        "id": "D1ZdUgbmNY8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAkPbeVbOOCd",
        "outputId": "0747557b-5558-4c8a-bad9-5e6d387ef7f4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(227845, 29)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como temos 29 colunas, teremos 29 neurônios."
      ],
      "metadata": {
        "id": "JwPk1TQwK59b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import exp"
      ],
      "metadata": {
        "id": "l1BcKcimq6UJ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weight = np.random.randn(29)"
      ],
      "metadata": {
        "id": "yPim6qMurmoK"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n=X_train.shape[1]\n",
        "n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGQQQC5sgEWg",
        "outputId": "65fa173e-88b5-44d0-f677-965c73893dcf"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a =np.array([[1,2,3],[4,5,6]])\n",
        "a.shape\n",
        "print(a.reshape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUShDattt06k",
        "outputId": "fffe83a7-92d5-42ae-98d8-d5a1bd53b957"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<built-in method reshape of numpy.ndarray object at 0x7f7ac4d21e10>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w1 = np.random.rand(29)  # Layer 1\n",
        "b1 = np.random.rand(4)\n",
        "w1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9UhoaZLfDLd",
        "outputId": "2324e5eb-55f6-432e-bacd-4f952b141ede"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.61925633, 0.0289573 , 0.99356462, 0.56778625, 0.5175795 ,\n",
              "       0.10903177, 0.60236273, 0.24932146, 0.78990484, 0.29313812,\n",
              "       0.78941332, 0.84284009, 0.238793  , 0.99809814, 0.53159688,\n",
              "       0.86515271, 0.4553287 , 0.49993807, 0.15257722, 0.62862918,\n",
              "       0.72494896, 0.82599632, 0.18174775, 0.62807145, 0.8881294 ,\n",
              "       0.95315586, 0.90792766, 0.78545374, 0.18390968])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "-w1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xG8N_j9Pzk63",
        "outputId": "42bb6058-1436-4ba3-9992-806ac55e5bdb"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.61925633, -0.0289573 , -0.99356462, -0.56778625, -0.5175795 ,\n",
              "       -0.10903177, -0.60236273, -0.24932146, -0.78990484, -0.29313812,\n",
              "       -0.78941332, -0.84284009, -0.238793  , -0.99809814, -0.53159688,\n",
              "       -0.86515271, -0.4553287 , -0.49993807, -0.15257722, -0.62862918,\n",
              "       -0.72494896, -0.82599632, -0.18174775, -0.62807145, -0.8881294 ,\n",
              "       -0.95315586, -0.90792766, -0.78545374, -0.18390968])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Inicialização"
      ],
      "metadata": {
        "id": "dtFzeMTPglv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Inicialização():\n",
        "  def __init__(self):\n",
        "    pass\n",
        " \n",
        "  def weight_bias(self, n_features, X):\n",
        "    w1 = np.random.rand(n_features , X.shape[1])        # Layer 1\n",
        "    b1 = np.random.rand(n_features)\n",
        "\n",
        "    w2 = np.random.rand(n_features , X.shape[1])        # Layer 2\n",
        "    b2 = np.random.rand(n_features)\n",
        "\n",
        "    weight_and_bias = {\"weight1\": w1, \"bias1\": b1, \"weight2\": w2, \"bias2\": b2}\n",
        "    return weight_and_bias\n"
      ],
      "metadata": {
        "id": "-n6M94Yp1ps7"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Foward\n"
      ],
      "metadata": {
        "id": "Rmaf-jAysJPl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Foward():\n",
        "\n",
        "  def __init__(self):\n",
        "    pass\n",
        "        \n",
        "  def propagation(self, x, weight_bias):\n",
        "   \n",
        "\n",
        "    z1 = np.dot(weight_bias[\"weight1\"], x) + weight_bias[\"bias1\"]        #Calculando os outputs da primeira camada\n",
        "    z1_active = np.tanh(z1)  # O próximo passo é passar esses outputs ,da primeira camada, na função de ativação\n",
        "\n",
        "    z2 = np.dot(weight_bias[\"weight2\"], z1) + weight_bias[\"bias2\"]       # Mesma descrição do comentário acima\n",
        "    z2_active = self.sigmoid(z2)\n",
        "    \n",
        "    active = {\"z1\": z1 ,\"z2\": z2 , \"z1_active\": z1_active , \"z2_active\": z2_active}\n",
        "    return z2_active, active\n",
        "\n",
        "\n",
        "  def sigmoid(self,z):\n",
        "      return 1/(1+np.exp(-z))"
      ],
      "metadata": {
        "id": "NA9EKCbhRs4U"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Cost"
      ],
      "metadata": {
        "id": "OwJY68__sL4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class Cost():\n",
        "  \n",
        "#   def __init__(self) -> None:\n",
        "#       pass\n",
        "def Entropy(active_layers, y):\n",
        "  #Tem-se muitas funções para calcular o custo, e para cada modelo tem um tipo ideal de função. \n",
        "  #Por falta de espertise, escolhi a que seria mais fácil de ser implementada. Cross-Entropy-Loss-Function\n",
        "  #Com uma rápida pesquisada pude perceber que essa função tem importância significativa na Teoria de Informação\n",
        "  #Mas basicamente calcula-se a probabilidade da distribuição dos dados.  https://towardsdatascience.com/cross-entropy-loss-function-f38c4ec8643e\n",
        "  # Compute the cross-entropy cost\n",
        "  entropy_cost = np.multiply(np.log(active_layers), x) + np.multiply((1-x), np.log(1 - active_layers))\n",
        "\n",
        "  cost = - np.sum(entropy_cost) / x.shape[1]\n",
        " \n",
        "  cost1 = float(np.squeeze(cost))\n",
        "                                  \n",
        "  return cost1"
      ],
      "metadata": {
        "id": "C36aFKr_YZEy"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Backward"
      ],
      "metadata": {
        "id": "d45LPEBrsO4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Backward():\n",
        "  def __init__(self):   #I belive that will not necessary index and xi\n",
        "    pass\n",
        "  \n",
        " \n",
        "  def backpropagation(self, active_layers, weight_and_bias, x, y):\n",
        "    \n",
        "    n_features =29\n",
        "#Para a última camada (layer) faremos a diferença entre o valor da camada na função de ativação\n",
        "#(isso quer dizer ajustá-la num intervalo binário) menos o valor esperado, esses dados são a \"diferença mínima\" da layer\n",
        "    dz2 = active_layers[\"z2_active\"] - y\n",
        "#Para os pesos multiplicamos esse \"Diferença mínima\" pela primeira camada transposta (aplicada na função de ativação) e dvidimos pela quantidade de neurônios\n",
        "    dw2 = (dz2*active_layers[\"z1_active\"])/n_features\n",
        "#Para os bias tiramos a média dessa \"Diferença mz2_activeínima\"\n",
        "    db2 = (np.sum(dz2, keepdims=True)) #Vale ressaltar que o \"keepdims\" mantém em formato de array\n",
        "\n",
        "    dz1 = np.multiply(np.dot(weight_and_bias[\"weight2\"].T, dz2), 1 - np.power(active_layers[\"z1_active\"], 2))\n",
        "    dw1 = (1/n_features)*np.dot(dz1, x.T)\n",
        "    db1 = (1/n_features)*np.sum(dz1, keepdims=True)\n",
        "\n",
        "    gradient = {\"derivate_w1\": dw1, \"derivate_b1\": db1, \"derivate_w2\": dw2, \"derivate_b2\": db2}\n",
        "\n",
        "    return gradient\n",
        "\n"
      ],
      "metadata": {
        "id": "ahYpr4ZY1wxL"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Gradient"
      ],
      "metadata": {
        "id": "9DRcQJRLsRWb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Gradient():\n",
        "  def __init__(self) -> None:\n",
        "      pass\n",
        "\n",
        "  def descent(self, gradient, weight_and_bias, learning_rate=0.01):\n",
        "    w1 = weight_and_bias['weight1']\n",
        "    b1 = weight_and_bias['bias1']\n",
        "    w2 = weight_and_bias['weight2']\n",
        "    b2 = weight_and_bias['bias2']\n",
        "   \n",
        "    dw1 = gradient['derivate_w1']\n",
        "    db1 = gradient['derivate_b1']\n",
        "    dw2 = gradient['derivate_w2']\n",
        "    db2 = gradient['derivate_b2']\n",
        "\n",
        "    w1 = w1 - learning_rate * dw1\n",
        "    b1 = b1 - learning_rate * db1\n",
        "    w2 = w2 - learning_rate * dw2\n",
        "    b2 = b2 - learning_rate * db2\n",
        "    \n",
        "    weight_and_bias = {\"weight1\": w1, \"bias1\": b1, \"weight2\": w2, \"bias2\": b2}\n",
        "    \n",
        "    return weight_and_bias"
      ],
      "metadata": {
        "id": "pFDQ7r9iYch2"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Perceptron\n"
      ],
      "metadata": {
        "id": "6wfHMHmUsyIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Perceptron():\n",
        "\n",
        "  def __init__(self) -> None:\n",
        "\n",
        "      self.weight_and_bias = None\n",
        "      pass\n",
        "        \n",
        "  def fit(self, x, y, num_iterations = 1000):\n",
        "    n_features = x.shape[1]\n",
        "    init = Inicialização()\n",
        "    self.weight_and_bias = init.weight_bias(n_features, x)\n",
        "    \n",
        "    w1 = self.weight_and_bias['weight1']\n",
        "    b1 = self.weight_and_bias['bias1']\n",
        "    w2 = self.weight_and_bias['weight2']\n",
        "    b2 = self.weight_and_bias['bias2']\n",
        "    \n",
        "    forward = Foward()\n",
        "    backward = Backward()\n",
        "    gradient = Gradient()\n",
        "\n",
        "    for i in range(0, num_iterations):\n",
        "\n",
        "      z2_active, active = forward.propagation(x, self.weight_and_bias)\n",
        "\n",
        "      cost = Entropy(z2_active, y)\n",
        "      grads = backward.backpropagation(active, self.weight_and_bias, x, y)\n",
        "      self.weight_and_bias = gradient.descent(grads, self.weight_and_bias)\n",
        "\n",
        "    return self.weight_and_bias\n",
        "\n",
        "  def prediction(self, x):\n",
        "    forward = Foward()\n",
        "    weight_bias, active = forward.propagation(x, self.weight_and_bias)\n",
        "    predictions = np.round(active[\"z2_active\"])\n",
        "    \n",
        "    return predictions"
      ],
      "metadata": {
        "id": "3tQSxmybiAkQ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "perceptron = Perceptron()"
      ],
      "metadata": {
        "id": "RkcyocJi0FLt"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# perceptron.fit(X_train, y_test)"
      ],
      "metadata": {
        "id": "bYKm2fXfz9wg"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nesse modelo, selecionarei somente uma parte do dataset para predição. Especificamente (29,29) do dataset."
      ],
      "metadata": {
        "id": "8oVEEvQ1ooWE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Selecionando aleatoriamente uma parte do data set"
      ],
      "metadata": {
        "id": "fRVNMC-Toz0T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate random integer values\n",
        "from random import seed\n",
        "from random import randint\n",
        "# seed random number generator\n",
        "random_value_test = randint(0,len(X_test))"
      ],
      "metadata": {
        "id": "RvOyFQjpozH3"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X_test = X_test[random_value_test:random_value_test+29]"
      ],
      "metadata": {
        "id": "F5WPU1NsqHXk"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# y_pred  =perceptron.prediction(X_test)"
      ],
      "metadata": {
        "id": "GscdQynLmiYI"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, r2_score, mean_squared_error"
      ],
      "metadata": {
        "id": "tnzZIBxIvGGW"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test = y_test[random_value_test:random_value_test+29]"
      ],
      "metadata": {
        "id": "TQgtutXCxYGi"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# erro = np.power(abs(y_pred - y_test),2)"
      ],
      "metadata": {
        "id": "wjWJPxqn6VAl"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mean_square = np.square(erro)\n",
        "# mean_square = np.sum(mean_square)"
      ],
      "metadata": {
        "id": "MpBBpase6BBX"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Não consegui terminar o perceptron. Fui muito ousado ao tentar usar as classes, fico muito triste por não conseguir concluir. Mas colocarei aqui minhas análises a cerca do que eu esperava. "
      ],
      "metadata": {
        "id": "APk4Rxnz9PJf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ao analisar o dataset, pude perceber que a quantidade de valores 1 é maior que 0. \n",
        "\n",
        "\n",
        "1.   0.0  ---->    284315\n",
        "2.   1.0  ---->     492"
      ],
      "metadata": {
        "id": "gTPoKFFe90IS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para treinar esse dataset é necessário selecionar a mesma quantidade de zeros e uns. Porque daria overfitting."
      ],
      "metadata": {
        "id": "m96M5aKN90C4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Perceptron by Sklearn"
      ],
      "metadata": {
        "id": "K6Sgb4R64yLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "xJUgG06J47PS"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label = 'Class'\n",
        "x = np.array(df.drop(label, axis=1))\n",
        "y = np.array(df[label])"
      ],
      "metadata": {
        "id": "eprLIQRY6GcE"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0 )"
      ],
      "metadata": {
        "id": "jyRK7zqi7vu3"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "percep_x = Perceptron()"
      ],
      "metadata": {
        "id": "eH_ILYxI8EUq"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "percep_x.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "fbdppxkv8HrO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba389df6-19b4-4207-cce5-168af4e5724e"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Perceptron()"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = percep_x.predict(X_test)"
      ],
      "metadata": {
        "id": "RaxCztba8JYp"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def score_accuracy(array1, array2):\n",
        "#Basicamente essa função accuracy ela soma a quantidade de elementos iguais e divide pelo tamanho , no nosso caso, y_test\n",
        "  accuracy = np.sum(array1==array2) / len(array1)\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "NLeVjKdU8LVe"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_multiclass = classification_report(y_test, y_pred)\n",
        "\n",
        "print(metrics_multiclass)"
      ],
      "metadata": {
        "id": "hOA7aJZp8M0-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7611034-06d6-45dd-c2bb-77d752bfc257"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      1.00      1.00     56861\n",
            "         1.0       0.86      0.38      0.52       101\n",
            "\n",
            "    accuracy                           1.00     56962\n",
            "   macro avg       0.93      0.69      0.76     56962\n",
            "weighted avg       1.00      1.00      1.00     56962\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos ver por esses dados, que o treino foi viciado, ou seja, o modelo está mais acostumado ao 0 do que o 1."
      ],
      "metadata": {
        "id": "5TtW5_Cx_liU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Undersampling"
      ],
      "metadata": {
        "id": "qo7wM5PoATuW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_new = df.copy()\n",
        "\n",
        "\n",
        "\n",
        "aux = df_new.query('Class == 1')\n",
        "\n",
        "X_1 = aux.drop('Class', axis = 1)\n",
        "\n",
        "X_1 = np.array(X_1)\n",
        "\n",
        "\n",
        "\n",
        "y_1 = aux[['Class', 'Amount']]\n",
        "\n",
        "y_1 = y_1.drop('Amount', axis = 1)\n",
        "\n",
        "y_1 = np.array(y_1)"
      ],
      "metadata": {
        "id": "-PV8Vpt1DYtZ"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "value_zero = df.query(\"Class==0\")\n",
        "X_1 = value_zero.drop(\"Class\", axis =1)\n",
        "X_1 = np.array(X_1)\n",
        "\n",
        "Y_1 = value_zero[['Class', 'Amount']]\n",
        "Y_1 = Y_1.drop(\"Amount\", axis =1)\n",
        "Y_1 = np.array(Y_1)\n",
        "\n",
        "\n",
        "value_one = df.query(\"Class==1\")\n",
        "X_2 = value_one.drop(\"Class\", axis =1)\n",
        "X_2 = np.array(X_2)\n",
        "\n",
        "Y_2 = value_one[['Class', 'Amount']]\n",
        "Y_2 = Y_2.drop(\"Amount\", axis =1)\n",
        "Y_2 = np.array(Y_2)"
      ],
      "metadata": {
        "id": "ZUwZTZKK8ObI"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "80% dos dados para treinar e os outros 20% para testar"
      ],
      "metadata": {
        "id": "Jog6um7FEzzA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_2 = np.concatenate((X_1[:394], X_2[:394]))\n",
        "\n",
        "X_test_2 = np.concatenate((X_1[:98], X_2[:98]))\n",
        "\n",
        "\n",
        "\n",
        "Y_train_2 = np.concatenate((Y_1[:394], Y_2[:394]))\n",
        "\n",
        "Y_test_2 = np.concatenate((Y_1[:98], Y_2[:98]))"
      ],
      "metadata": {
        "id": "eqPOAe1xAQ3-"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "percep_x = Perceptron()"
      ],
      "metadata": {
        "id": "UJ1XXxwUB7cY"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "percep_x.fit(X_train_2, Y_train_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_OfO0Y6eFNfv",
        "outputId": "30e8c794-3575-4258-e698-a1248bc633a0"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Perceptron()"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = percep_x.predict(X_test_2)"
      ],
      "metadata": {
        "id": "QLSC_XbVF3de"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_multiclass = classification_report(Y_test_2, y_pred)\n",
        "\n",
        "print(metrics_multiclass)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nyix7TqFCHUm",
        "outputId": "17b6597d-b8fd-4dae-cbfe-eb6f60163cb2"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.88      1.00      0.93        98\n",
            "         1.0       1.00      0.86      0.92        98\n",
            "\n",
            "    accuracy                           0.93       196\n",
            "   macro avg       0.94      0.93      0.93       196\n",
            "weighted avg       0.94      0.93      0.93       196\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tensor Flow"
      ],
      "metadata": {
        "id": "KwAnkkm8HHc7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importando biblioteca"
      ],
      "metadata": {
        "id": "nMTFRpKYHlLh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras"
      ],
      "metadata": {
        "id": "biQoRL4rHItL"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shape = X_train_2.shape"
      ],
      "metadata": {
        "id": "UQHp-xN9IUs7"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential()  # Cria uma rede neural sequencial - feed foward\n",
        "\n",
        "model.add(keras.layers.Dense(2, activation='relu', input_shape=X_train_2[0].shape)) # primeira camada oculta com 2 neurônios\n",
        "\n",
        "model.add(keras.layers.Dropout(0.2))\n",
        "\n",
        "model.add(keras.layers.Dense(1, activation= 'relu')) # camada de saída com 1 neurônio com ativação sigmoid\n",
        "\n",
        "\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FVU48LsHj6Y",
        "outputId": "11e19b41-78cf-4c96-fcff-db52f0d080f0"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_2 (Dense)             (None, 2)                 60        \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 2)                 0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 3         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 63\n",
            "Trainable params: 63\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Treino o a rede neural"
      ],
      "metadata": {
        "id": "KEBoxi2CJlS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=\"sgd\", loss=\"categorical_crossentropy\", metrics=[\"binary_accuracy\"])"
      ],
      "metadata": {
        "id": "fot74dT3HwRd"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_2.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36biqPyJLl1u",
        "outputId": "ee48c5c1-611e-4eca-a4c0-367373a44f02"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(788, 29)"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 500  \n",
        "epochs = 500 #Quantidade de iterações\n",
        "\n",
        "history  = model.fit(X_train_2, Y_train_2, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(X_test_2, Y_test_2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "at5A2YnXKMKv",
        "outputId": "13ce291d-f724-4032-e5fc-075b7e28e774"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "2/2 [==============================] - 1s 227ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 2/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 3/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 4/500\n",
            "2/2 [==============================] - 0s 26ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 5/500\n",
            "2/2 [==============================] - 0s 25ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 6/500\n",
            "2/2 [==============================] - 0s 27ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 7/500\n",
            "2/2 [==============================] - 0s 24ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 8/500\n",
            "2/2 [==============================] - 0s 27ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 9/500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 10/500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 11/500\n",
            "2/2 [==============================] - 0s 25ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 12/500\n",
            "2/2 [==============================] - 0s 25ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 13/500\n",
            "2/2 [==============================] - 0s 25ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 14/500\n",
            "2/2 [==============================] - 0s 27ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 15/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 16/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 17/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 18/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 19/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 20/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 21/500\n",
            "2/2 [==============================] - 0s 26ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 22/500\n",
            "2/2 [==============================] - 0s 25ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 23/500\n",
            "2/2 [==============================] - 0s 26ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 24/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 25/500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 26/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 27/500\n",
            "2/2 [==============================] - 0s 47ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 28/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 29/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 30/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 31/500\n",
            "2/2 [==============================] - 0s 27ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 32/500\n",
            "2/2 [==============================] - 0s 27ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 33/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 34/500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 35/500\n",
            "2/2 [==============================] - 0s 25ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 36/500\n",
            "2/2 [==============================] - 0s 26ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 37/500\n",
            "2/2 [==============================] - 0s 27ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 38/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 39/500\n",
            "2/2 [==============================] - 0s 25ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 40/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 41/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 42/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 43/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 44/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 45/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 46/500\n",
            "2/2 [==============================] - 0s 26ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 47/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 48/500\n",
            "2/2 [==============================] - 0s 26ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 49/500\n",
            "2/2 [==============================] - 0s 25ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 50/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 51/500\n",
            "2/2 [==============================] - 0s 25ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 52/500\n",
            "2/2 [==============================] - 0s 25ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 53/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 54/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 55/500\n",
            "2/2 [==============================] - 0s 26ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 56/500\n",
            "2/2 [==============================] - 0s 27ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 57/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 58/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 59/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 60/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 61/500\n",
            "2/2 [==============================] - 0s 27ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 62/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 63/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 64/500\n",
            "2/2 [==============================] - 0s 27ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 65/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 66/500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 67/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 68/500\n",
            "2/2 [==============================] - 0s 26ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 69/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 70/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 71/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 72/500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 73/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 74/500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 75/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 76/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 77/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 78/500\n",
            "2/2 [==============================] - 0s 27ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 79/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 80/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 81/500\n",
            "2/2 [==============================] - 0s 47ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 82/500\n",
            "2/2 [==============================] - 0s 27ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 83/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 84/500\n",
            "2/2 [==============================] - 0s 26ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 85/500\n",
            "2/2 [==============================] - 0s 27ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 86/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 87/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 88/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 89/500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 90/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 91/500\n",
            "2/2 [==============================] - 0s 27ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 92/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 93/500\n",
            "2/2 [==============================] - 0s 27ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 94/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 95/500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 96/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 97/500\n",
            "2/2 [==============================] - 0s 27ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 98/500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 99/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 100/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 101/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 102/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 103/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 104/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 105/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 106/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 107/500\n",
            "2/2 [==============================] - 0s 27ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 108/500\n",
            "2/2 [==============================] - 0s 26ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 109/500\n",
            "2/2 [==============================] - 0s 26ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 110/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 111/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 112/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 113/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 114/500\n",
            "2/2 [==============================] - 0s 27ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 115/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 116/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 117/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 118/500\n",
            "2/2 [==============================] - 0s 27ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 119/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 120/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 121/500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 122/500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 123/500\n",
            "2/2 [==============================] - 0s 49ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 124/500\n",
            "2/2 [==============================] - 0s 26ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 125/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 126/500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 127/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 128/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 129/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 130/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 131/500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 132/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 133/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 134/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 135/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 136/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 137/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 138/500\n",
            "2/2 [==============================] - 0s 27ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 139/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 140/500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 141/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 142/500\n",
            "2/2 [==============================] - 0s 27ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 143/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 144/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 145/500\n",
            "2/2 [==============================] - 0s 27ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 146/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 147/500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 148/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 149/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 150/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 151/500\n",
            "2/2 [==============================] - 0s 27ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 152/500\n",
            "2/2 [==============================] - 0s 27ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 153/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 154/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 155/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 156/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 157/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 158/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 159/500\n",
            "2/2 [==============================] - 0s 26ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 160/500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 161/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 162/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 163/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 164/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 165/500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 166/500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 167/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 168/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 169/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 170/500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 171/500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 172/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 173/500\n",
            "2/2 [==============================] - 0s 27ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 174/500\n",
            "2/2 [==============================] - 0s 26ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 175/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 176/500\n",
            "2/2 [==============================] - 0s 27ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 177/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 178/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 179/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 180/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 181/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 182/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 183/500\n",
            "2/2 [==============================] - 0s 27ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 184/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 185/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 186/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 187/500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 188/500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 189/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 190/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 191/500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 192/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 193/500\n",
            "2/2 [==============================] - 0s 27ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 194/500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 195/500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 196/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 197/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 198/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 199/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 200/500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 201/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 202/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 203/500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 204/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 205/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 206/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 207/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 208/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 209/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 210/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 211/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 212/500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 213/500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 214/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 215/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 216/500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 217/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 218/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 219/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 220/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 221/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 222/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 223/500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 224/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 225/500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 226/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 227/500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 228/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 229/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 230/500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 231/500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 232/500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 233/500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 234/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 235/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 236/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 237/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 238/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 239/500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 240/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 241/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 242/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 243/500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 244/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 245/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 246/500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 247/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 248/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 249/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 250/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 251/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 252/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 253/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 254/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 255/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 256/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 257/500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 258/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 259/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 260/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 261/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 262/500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 263/500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 264/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 265/500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 266/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 267/500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 268/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 269/500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 270/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 271/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 272/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 273/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 274/500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 275/500\n",
            "2/2 [==============================] - 0s 28ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 276/500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 277/500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 278/500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 279/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 280/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 281/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 282/500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 283/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 284/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 285/500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 286/500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 287/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 288/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 289/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 290/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 291/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 292/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 293/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 294/500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 295/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 296/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 297/500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 298/500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 299/500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 300/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 301/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 302/500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 303/500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 304/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 305/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 306/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 307/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 308/500\n",
            "2/2 [==============================] - 0s 44ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 309/500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 310/500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 311/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 312/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 313/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 314/500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 315/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 316/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 317/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 318/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 319/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 320/500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 321/500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 322/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 323/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 324/500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 325/500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 326/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 327/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 328/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 329/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 330/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 331/500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 332/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 333/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 334/500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 335/500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 336/500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 337/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 338/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 339/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 340/500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 341/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 342/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 343/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 344/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 345/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 346/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 347/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 348/500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 349/500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 350/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 351/500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 352/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 353/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 354/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 355/500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 356/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 357/500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 358/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 359/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 360/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 361/500\n",
            "2/2 [==============================] - 0s 45ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 362/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 363/500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 364/500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 365/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 366/500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 367/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 368/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 369/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 370/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 371/500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 372/500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 373/500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 374/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 375/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 376/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 377/500\n",
            "2/2 [==============================] - 0s 47ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 378/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 379/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 380/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 381/500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 382/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 383/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 384/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 385/500\n",
            "2/2 [==============================] - 0s 30ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 386/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 387/500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 388/500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 389/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 390/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 391/500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 392/500\n",
            "2/2 [==============================] - 0s 29ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 393/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 394/500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 395/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 396/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 397/500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 398/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 399/500\n",
            "2/2 [==============================] - 0s 46ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 400/500\n",
            "2/2 [==============================] - 0s 42ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 401/500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 402/500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 403/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 404/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 405/500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 406/500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 407/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 408/500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 409/500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 410/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 411/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 412/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 413/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 414/500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 415/500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 416/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 417/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 418/500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 419/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 420/500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 421/500\n",
            "2/2 [==============================] - 0s 51ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 422/500\n",
            "2/2 [==============================] - 0s 31ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 423/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 424/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 425/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 426/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 427/500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 428/500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 429/500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 430/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 431/500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 432/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 433/500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 434/500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 435/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 436/500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 437/500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 438/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 439/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 440/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 441/500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 442/500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 443/500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 444/500\n",
            "2/2 [==============================] - 0s 43ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 445/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 446/500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 447/500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 448/500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 449/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 450/500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 451/500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 452/500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 453/500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 454/500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 455/500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 456/500\n",
            "2/2 [==============================] - 0s 36ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 457/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 458/500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 459/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 460/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 461/500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 462/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 463/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 464/500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 465/500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 466/500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 467/500\n",
            "2/2 [==============================] - 0s 46ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 468/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 469/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 470/500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 471/500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 472/500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 473/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 474/500\n",
            "2/2 [==============================] - 0s 37ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 475/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 476/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 477/500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 478/500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 479/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 480/500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 481/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 482/500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 483/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 484/500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 485/500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 486/500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 487/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 488/500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 489/500\n",
            "2/2 [==============================] - 0s 39ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 490/500\n",
            "2/2 [==============================] - 0s 40ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 491/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 492/500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 493/500\n",
            "2/2 [==============================] - 0s 41ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 494/500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 495/500\n",
            "2/2 [==============================] - 0s 33ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 496/500\n",
            "2/2 [==============================] - 0s 48ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 497/500\n",
            "2/2 [==============================] - 0s 38ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 498/500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 499/500\n",
            "2/2 [==============================] - 0s 35ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n",
            "Epoch 500/500\n",
            "2/2 [==============================] - 0s 34ms/step - loss: nan - binary_accuracy: 0.5000 - val_loss: nan - val_binary_accuracy: 0.5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "matplotlib.rcParams['figure.dpi'] = 150\n",
        "\n",
        "\n",
        "\n",
        "# Plot acurácia de treino e teste\n",
        "\n",
        "plt.plot(history.history['binary_accuracy'])\n",
        "\n",
        "plt.plot(history.history['val_binary_accuracy'])\n",
        "\n",
        "plt.title('Acurácia com Tensor FLow')\n",
        "\n",
        "plt.ylabel('Acurácia')\n",
        "\n",
        "plt.xlabel('Ciclos')\n",
        "\n",
        "plt.legend(['Treino', 'Teste'], loc='upper left')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 593
        },
        "id": "-DaXL1rKLc5C",
        "outputId": "d91c2f5e-6563-4545-f61d-a44d8e383232"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 900x600 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "eVN_h7iwN5zs"
      },
      "execution_count": 70,
      "outputs": []
    }
  ]
}